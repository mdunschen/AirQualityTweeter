{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/01/2017 16:00 (48.0, '(6 Moderate)')\n",
      "27/01/2017 15:00 (47.0, '(5 Moderate)')\n",
      "27/01/2017 14:00 (47.0, '(5 Moderate)')\n",
      "27/01/2017 13:00 (46.0, '(5 Moderate)')\n",
      "27/01/2017 12:00 (46.0, '(5 Moderate)')\n",
      "27/01/2017 11:00 (45.0, '(5 Moderate)')\n",
      "27/01/2017 10:00 (44.0, '(5 Moderate)')\n",
      "27/01/2017 09:00 (43.0, '(5 Moderate)')\n",
      "27/01/2017 08:00 (42.0, '(5 Moderate)')\n",
      "27/01/2017 07:00 (40.0, '(4 Moderate)')\n",
      "27/01/2017 06:00 (39.0, '(4 Moderate)')\n",
      "27/01/2017 05:00 (37.0, '(4 Moderate)')\n",
      "27/01/2017 04:00 (37.0, '(4 Moderate)')\n",
      "27/01/2017 03:00 (37.0, '(4 Moderate)')\n",
      "27/01/2017 02:00 (37.0, '(4 Moderate)')\n",
      "27/01/2017 01:00 (36.0, '(4 Moderate)')\n",
      "26/01/2017 24:00 (35.0, '(3 Low)')\n",
      "26/01/2017 23:00 (34.0, '(3 Low)')\n",
      "26/01/2017 22:00 (34.0, '(3 Low)')\n",
      "26/01/2017 21:00 (34.0, '(3 Low)')\n",
      "26/01/2017 20:00 (33.0, '(3 Low)')\n",
      "26/01/2017 19:00 (33.0, '(3 Low)')\n",
      "26/01/2017 18:00 (32.0, '(3 Low)')\n",
      "26/01/2017 17:00 (31.0, '(3 Low)')\n",
      "26/01/2017 16:00 (31.0, '(3 Low)')\n",
      "26/01/2017 15:00 (30.0, '(3 Low)')\n",
      "26/01/2017 14:00 (29.0, '(3 Low)')\n",
      "26/01/2017 13:00 (29.0, '(3 Low)')\n",
      "26/01/2017 12:00 (28.0, '(3 Low)')\n",
      "26/01/2017 11:00 (27.0, '(3 Low)')\n",
      "26/01/2017 10:00 (27.0, '(3 Low)')\n",
      "26/01/2017 09:00 (26.0, '(3 Low)')\n",
      "26/01/2017 08:00 (26.0, '(3 Low)')\n",
      "26/01/2017 07:00 (25.0, '(3 Low)')\n",
      "26/01/2017 06:00 (25.0, '(3 Low)')\n",
      "26/01/2017 05:00 (23.0, '(2 Low)')\n",
      "26/01/2017 04:00 (23.0, '(2 Low)')\n",
      "26/01/2017 03:00 (22.0, '(2 Low)')\n",
      "26/01/2017 02:00 (22.0, '(2 Low)')\n",
      "26/01/2017 01:00 (21.0, '(2 Low)')\n",
      "25/01/2017 24:00 (21.0, '(2 Low)')\n",
      "25/01/2017 23:00 (20.0, '(2 Low)')\n",
      "25/01/2017 22:00 (20.0, '(2 Low)')\n",
      "25/01/2017 21:00 (20.0, '(2 Low)')\n",
      "25/01/2017 20:00 (19.0, '(2 Low)')\n",
      "25/01/2017 19:00 (19.0, '(2 Low)')\n",
      "25/01/2017 18:00 (19.0, '(2 Low)')\n",
      "25/01/2017 17:00 (19.0, '(2 Low)')\n",
      "25/01/2017 16:00 (20.0, '(2 Low)')\n",
      "25/01/2017 15:00 (20.0, '(2 Low)')\n",
      "urls =  ['https://goo.gl/ZpELjS']\n",
      "27/01/2017, 17:00 (μgm⁻³)\n",
      "NO₂: 50 (1 Low)\n",
      "O₃: 19 (1 Low)\n",
      "PM₁₀₀: 54 (4 Moderate)\n",
      "PM₂₅: 49 (6 Moderate)\n",
      "SO₂: 9 (1 Low)\n",
      "https://goo.gl/ZpELjS\n",
      "urls =  ['http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf)']\n",
      "@lpoolcouncil @DefraUKAir @LiverpoolFoE: PM₁₀₀ 5h above @WHO guide (50μgm⁻³ 24h-mean http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf) #airpollution #liverpool\n",
      "urls =  ['http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf)']\n",
      "@lpoolcouncil @DefraUKAir @LiverpoolFoE: PM₂₅ 34h above @WHO guide (25μgm⁻³ 24h-mean http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf) #airpollution #liverpool\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import urllib\n",
    "import urllib.request as request\n",
    "import re\n",
    "import html\n",
    "import sys, os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "sys.path.append(\"C:\\\\Program Files\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\")\n",
    "import tweepy\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "urlstr = \"https://uk-air.defra.gov.uk/latest/currentlevels?view=site#L\"\n",
    "shorturlstr = \"https://goo.gl/ZpELjS\"\n",
    "\n",
    "urlWHO = \"http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf\"\n",
    "\n",
    "sitename = b'Liverpool'\n",
    "\n",
    "mgm3 = '\\u03BCgm\\u207B\\u00B3'\n",
    "O3, NO2, SO2, PM25, PM100 = \"O\\u2083\", \"NO\\u2082\", \"SO\\u2082\", \"PM\\u2082\\u2085\", \"PM\\u2081\\u2080\\u2080\"\n",
    "guides = {O3:100, NO2:200, SO2:20, PM25:25, PM100:50} # source: http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf  \n",
    "meansWHO = {O3:'8h', NO2:'1h', SO2:'10m', PM25:'24h', PM100:'24h'}\n",
    "meansDEFRA = {O3:'8h', NO2:'1h', SO2:'max 15m', PM25:'24h', PM100:'24h'}\n",
    "\n",
    "\n",
    "def tweet(status, replyto=None):\n",
    "    if not status:\n",
    "        return\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret = pickle.load(open(\"apikeys.bin\", \"rb\")) \n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', status)\n",
    "    print(\"urls = \", urls)\n",
    "    # take out all url texts from status for count, all urls count as 23\n",
    "    rstat = status\n",
    "    for u in urls:\n",
    "        rstat = rstat.replace(u, '')\n",
    "    nchars = len(rstat) + 23 * len(urls)\n",
    "    if nchars > 140:\n",
    "        print(\"Tweet too long\")\n",
    "        \n",
    "    #print(status)\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    try:\n",
    "        if replyto:\n",
    "            stat = api.update_status(status=status, in_reply_to_status_id=replyto.id)\n",
    "        else:\n",
    "            stat = api.update_status(status=status)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        stat = None\n",
    "    return stat\n",
    "    \n",
    "def compose(day, clock, reading):    \n",
    "    status = [\"%s, %s (%s)\" % (day, clock, mgm3)]\n",
    "    skeys = list(reading.keys())\n",
    "    skeys.sort()\n",
    "    for k in skeys:\n",
    "        status.append(\"%s: %.0f %s\" % (k, reading[k][0], reading[k][1]))\n",
    "    status.append(\"%s\" % shorturlstr)\n",
    "    status = '\\n'.join(status)\n",
    "    return status\n",
    "\n",
    "def composeAboveTweet(day, clock, above, origtweetstat):\n",
    "    def toDT(day, clock):\n",
    "        if clock == \"24:00\": # 27/01/2017 24:00 is in fact 28/01/2017 00:00\n",
    "            clock = \"00:00\"\n",
    "            day = (datetime.strptime(day, \"%d/%m/%Y\") + timedelta(hours=24)).strftime(\"%d/%m/%Y\")\n",
    "        return datetime.strptime(\"%s %s\" % (day, clock), \"%d/%m/%Y %H:%M\")\n",
    "\n",
    "    status = []\n",
    "    dtnow = toDT(day, clock)\n",
    "    for k in above:\n",
    "        # count hours above\n",
    "        #print(\"In composeAboveTweet\", k, above[k])\n",
    "        lday, lclock, lvalue = above[k][0]\n",
    "        if lday == day and lclock == clock:\n",
    "            stat = []\n",
    "            # count hours above\n",
    "            dtlast = dtnow\n",
    "            nhours = 1\n",
    "            for lday, lclock, lvalue in above[k][1:]:\n",
    "                if lday == day and lclock == clock:\n",
    "                    continue # skip duplicate entries\n",
    "                dt = toDT(lday, lclock)\n",
    "                \n",
    "                if (dtlast - dt) == timedelta(hours=1):\n",
    "                    nhours += 1\n",
    "                else:\n",
    "                    break\n",
    "                dtlast = dt\n",
    "            stat.append(\"@lpoolcouncil @DefraUKAir @LiverpoolFoE: %s %dh above @WHO guide (%.0f%s %s-mean %s) #airpollution #liverpool\" % \n",
    "                        (k, nhours, guides[k], mgm3, meansWHO[k], urlWHO))\n",
    "            if meansWHO[k] != meansDEFRA[k]:\n",
    "                stat.append(\"(Note #DEFRA data is %s mean)\" % meansDEFRA[k])            \n",
    "            status.append('\\n'.join(stat))\n",
    "    return status\n",
    "        \n",
    "\n",
    "\n",
    "def scrape():\n",
    "    f = request.urlopen(urlstr)\n",
    "\n",
    "    r = f.read()\n",
    "    g = re.search(b\".*<tr>.*(%s.*?)</tr>\" % sitename, r, re.DOTALL)\n",
    "    #print(g.group(1))\n",
    "\n",
    "    # split into <td></td>\n",
    "    row = g.group(1)\n",
    "    #print(\"row = %s\\n\" % row)\n",
    "\n",
    "    # date and time\n",
    "    dategroups = re.search(b\".*<td>(.*?)<br.*?>(.*?)</td>\", row, re.DOTALL)\n",
    "    day = dategroups.group(1).decode(\"utf-8\")\n",
    "    clock = dategroups.group(2).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "    # data\n",
    "    cols = re.findall(b\"<span.*?>(.*?)</span>\", row, re.DOTALL)\n",
    "    assert len(cols) == 5\n",
    "    units = [O3, NO2, SO2, PM25, PM100]\n",
    "    datanums = []\n",
    "    for v in cols:\n",
    "        if b' ' in v:\n",
    "            value = float(v[:v.index(b' ')])\n",
    "        else:\n",
    "            value = float(v[:v.index(b'&')])\n",
    "        nv = v.replace(b'&nbsp;', b' ')\n",
    "        ix = re.match(b\".*?(\\(.*?\\))\", nv).group(1)\n",
    "        datanums.append((value, ix.decode(\"utf-8\")))\n",
    "\n",
    "    reading = dict(zip(units, datanums))\n",
    "    return day, clock, reading\n",
    "\n",
    "def loadReadings():\n",
    "    fall = \"allreadings.bin\"\n",
    "    allreadings = deque()\n",
    "    if os.path.isfile(fall):\n",
    "        allreadings = pickle.load(open(fall, \"rb\"))\n",
    "    return allreadings\n",
    "\n",
    "def pickleReadings(allreading):\n",
    "    fall = \"allreadings.bin\"\n",
    "    pickle.dump(allreadings, open(fall, \"wb\"))\n",
    "    \n",
    "def compareWHO(allreadings):\n",
    "    above = {}\n",
    "    for (day, clock, reading) in allreadings:\n",
    "        for k in guides:\n",
    "            if reading[k][0] > guides[k]:\n",
    "                if k not in above:\n",
    "                    above[k] = []\n",
    "                above[k].append((day,clock, reading[k][0]))\n",
    "    return above\n",
    "\n",
    "\n",
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    stat = tweet(\"TTEESSTT\")\n",
    "    print(stat)\n",
    "    #tweet(\"In reply to: TEST3\", stat)\n",
    "\n",
    "else:\n",
    "    allreadings = loadReadings()\n",
    "    \n",
    "    # remove duplicate entries (could have come in while debugging)\n",
    "    ic = 0\n",
    "    while ic < len(allreadings):\n",
    "        r = allreadings[ic]\n",
    "        while allreadings.count(r) > 1:\n",
    "            allreadings.remove(r)\n",
    "        ic += 1 \n",
    "        \n",
    "    lastday, lastclock, lastreading = allreadings[0]\n",
    "    day, clock, reading = scrape()\n",
    "    if ((day, clock) != (lastday, lastclock)):\n",
    "        status = compose(day, clock, reading)\n",
    "        rtweet = tweet(status)\n",
    "\n",
    "        allreadings.appendleft((day, clock, reading))\n",
    "        pickleReadings(allreadings)\n",
    "\n",
    "        # compare with WHO recommendations\n",
    "        r = compareWHO(allreadings)\n",
    "        if r:\n",
    "            stats = composeAboveTweet(day, clock, r, rtweet)\n",
    "            for s in stats:\n",
    "                tweet(s, replyto=rtweet)\n",
    "    else:\n",
    "        print(\"Reading already known\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "d0 = datetime.strptime(\"27/01/2017 17:00\", \"%d/%m/%Y %H:%M\")\n",
    "d1 = datetime.strptime(\"27/01/2017 15:00\", \"%d/%m/%Y %H:%M\")\n",
    "\n",
    "print((d0-d1).total_seconds()/3600.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
