{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls =  ['https://goo.gl/ZpELjS']\n",
      "[{'code': 187, 'message': 'Status is a duplicate.'}]\n",
      "In composeAboveTweet PM₂₅ [('26/01/2017', '13:00', 29.0), ('26/01/2017', '12:00', 28.0), ('26/01/2017', '12:00', 28.0), ('26/01/2017', '12:00', 28.0), ('26/01/2017', '11:00', 27.0), ('26/01/2017', '11:00', 27.0), ('26/01/2017', '11:00', 27.0), ('26/01/2017', '11:00', 27.0), ('26/01/2017', '11:00', 27.0), ('26/01/2017', '10:00', 27.0), ('26/01/2017', '10:00', 27.0), ('26/01/2017', '10:00', 27.0), ('26/01/2017', '10:00', 27.0), ('26/01/2017', '10:00', 27.0), ('26/01/2017', '09:00', 26.0), ('26/01/2017', '09:00', 26.0), ('26/01/2017', '08:00', 26.0), ('26/01/2017', '08:00', 26.0), ('26/01/2017', '08:00', 26.0), ('24/01/2017', '14:00', 30.0), ('24/01/2017', '14:00', 30.0), ('24/01/2017', '14:00', 30.0)]\n",
      "urls =  ['http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf)']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import urllib\n",
    "import urllib.request as request\n",
    "import re\n",
    "import html\n",
    "import sys, os\n",
    "import pickle\n",
    "sys.path.append(\"C:\\\\Program Files\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\")\n",
    "import tweepy\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "urlstr = \"https://uk-air.defra.gov.uk/latest/currentlevels?view=site#L\"\n",
    "shorturlstr = \"https://goo.gl/ZpELjS\"\n",
    "\n",
    "urlWHO = \"http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf\"\n",
    "\n",
    "sitename = b'Liverpool'\n",
    "\n",
    "mgm3 = '\\u03BCgm\\u207B\\u00B3'\n",
    "O3, NO2, SO2, PM25, PM100 = \"O\\u2083\", \"NO\\u2082\", \"SO\\u2082\", \"PM\\u2082\\u2085\", \"PM\\u2081\\u2080\\u2080\"\n",
    "guides = {O3:100, NO2:200, SO2:20, PM25:25, PM100:50} # source: http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf  \n",
    "meansWHO = {O3:'8h', NO2:'1h', SO2:'10m', PM25:'24h', PM100:'24h'}\n",
    "meansDEFRA = {O3:'8h', NO2:'1h', SO2:'max 15m', PM25:'24h', PM100:'24h'}\n",
    "\n",
    "\n",
    "def tweet(status, replyto=None):\n",
    "    if not status:\n",
    "        return\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret = pickle.load(open(\"apikeys.bin\", \"rb\")) \n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', status)\n",
    "    print(\"urls = \", urls)\n",
    "    # take out all url texts from status for count, all urls count as 23\n",
    "    rstat = status\n",
    "    for u in urls:\n",
    "        rstat = rstat.replace(u, '')\n",
    "    nchars = len(rstat) + 23 * len(urls)\n",
    "    if nchars > 140:\n",
    "        print(\"Tweet too long\")\n",
    "        print(status)\n",
    "\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    try:\n",
    "        if replyto:\n",
    "            stat = api.update_status(status=status, in_reply_to_status_id=replyto.id)\n",
    "        else:\n",
    "            stat = api.update_status(status=status)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        stat = None\n",
    "    return stat\n",
    "    \n",
    "def compose(day, clock, reading):    \n",
    "    status = [\"%s, %s (%s)\" % (day, clock, mgm3)]\n",
    "    skeys = list(reading.keys())\n",
    "    skeys.sort()\n",
    "    for k in skeys:\n",
    "        status.append(\"%s: %.0f %s\" % (k, reading[k][0], reading[k][1]))\n",
    "    status.append(\"%s\" % shorturlstr)\n",
    "    status = '\\n'.join(status)\n",
    "    return status\n",
    "\n",
    "def composeAboveTweet(day, clock, above, origtweetstat):\n",
    "    status = []\n",
    "    for k in above:\n",
    "        # count hours above\n",
    "        print(\"In composeAboveTweet\", k, above[k])\n",
    "        lday, lclock, lvalue = above[k][0]\n",
    "        if lday == day and lclock == clock:\n",
    "            stat = []\n",
    "            # count hours above\n",
    "            nhours = 1\n",
    "            for lday, lclock, lvalue in above[k][1:]:\n",
    "                if int(lclock[:lclock.index(':')]) + nhours == int(clock[:clock.index(':')]):\n",
    "                    nhours += 1\n",
    "                else:\n",
    "                    break\n",
    "            stat.append(\"@lpoolcouncil @DefraUKAir @LiverpoolFoE: %s %dh above @WHO guide (%.0f%s %s-mean %s) #airpollution #liverpool\" % \n",
    "                        (k, nhours, guides[k], mgm3, meansWHO[k], urlWHO))\n",
    "            if meansWHO[k] != meansDEFRA[k]:\n",
    "                stat.append(\"(Note #DEFRA data is %s mean)\" % meansDEFRA[k])            \n",
    "            status.append('\\n'.join(stat))\n",
    "    return status\n",
    "        \n",
    "\n",
    "\n",
    "def scrape():\n",
    "    f = request.urlopen(urlstr)\n",
    "\n",
    "    r = f.read()\n",
    "    g = re.search(b\".*<tr>.*(%s.*?)</tr>\" % sitename, r, re.DOTALL)\n",
    "    #print(g.group(1))\n",
    "\n",
    "    # split into <td></td>\n",
    "    row = g.group(1)\n",
    "    #print(\"row = %s\\n\" % row)\n",
    "\n",
    "    # date and time\n",
    "    dategroups = re.search(b\".*<td>(.*?)<br.*?>(.*?)</td>\", row, re.DOTALL)\n",
    "    day = dategroups.group(1).decode(\"utf-8\")\n",
    "    clock = dategroups.group(2).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "    # data\n",
    "    cols = re.findall(b\"<span.*?>(.*?)</span>\", row, re.DOTALL)\n",
    "    assert len(cols) == 5\n",
    "    units = [O3, NO2, SO2, PM25, PM100]\n",
    "    datanums = []\n",
    "    for v in cols:\n",
    "        if b' ' in v:\n",
    "            value = float(v[:v.index(b' ')])\n",
    "        else:\n",
    "            value = float(v[:v.index(b'&')])\n",
    "        nv = v.replace(b'&nbsp;', b' ')\n",
    "        ix = re.match(b\".*?(\\(.*?\\))\", nv).group(1)\n",
    "        datanums.append((value, ix.decode(\"utf-8\")))\n",
    "\n",
    "    reading = dict(zip(units, datanums))\n",
    "    return day, clock, reading\n",
    "\n",
    "def loadReadings():\n",
    "    fall = \"allreadings.bin\"\n",
    "    allreadings = deque()\n",
    "    if os.path.isfile(fall):\n",
    "        allreadings = pickle.load(open(fall, \"rb\"))\n",
    "    return allreadings\n",
    "\n",
    "def pickleReadings(allreading):\n",
    "    fall = \"allreadings.bin\"\n",
    "    pickle.dump(allreadings, open(fall, \"wb\"))\n",
    "    \n",
    "def compareWHO(allreadings):\n",
    "    above = {}\n",
    "    for (day, clock, reading) in allreadings:\n",
    "        for k in guides:\n",
    "            if reading[k][0] > guides[k]:\n",
    "                if k not in above:\n",
    "                    above[k] = []\n",
    "                above[k].append((day,clock, reading[k][0]))\n",
    "    return above\n",
    "\n",
    "\n",
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    stat = tweet(\"TTEESSTT\")\n",
    "    print(stat)\n",
    "    #tweet(\"In reply to: TEST3\", stat)\n",
    "\n",
    "else:\n",
    "    day, clock, reading = scrape()\n",
    "    status = compose(day, clock, reading)\n",
    "    rtweet = tweet(status)\n",
    "\n",
    "    allreadings = loadReadings()\n",
    "    allreadings.appendleft((day, clock, reading))\n",
    "    pickleReadings(allreadings)\n",
    "\n",
    "    # compare with WHO recommendations\n",
    "    r = compareWHO(allreadings)\n",
    "    if r:\n",
    "        stats = composeAboveTweet(day, clock, r, rtweet)\n",
    "        for s in stats:\n",
    "            tweet(s, replyto=rtweet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
