{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls =  ['https://goo.gl/ZpELjS']\n"
     ]
    },
    {
     "ename": "TweepError",
     "evalue": "[{'code': 187, 'message': 'Status is a duplicate.'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-a03e3566f411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreading\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreading\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mrtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mallreadings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadReadings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-a03e3566f411>\u001b[0m in \u001b[0;36mtweet\u001b[0;34m(status, replyto)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_reply_to_status_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplyto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tweepy\\api.py\u001b[0m in \u001b[0;36mupdate_status\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mallowed_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in_reply_to_status_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'long'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'source'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'place_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'display_coordinates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mrequire_auth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         )(post_data=post_data, *args, **kwargs)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmedia_upload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[1;31m# Set pagination mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: [{'code': 187, 'message': 'Status is a duplicate.'}]"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import urllib\n",
    "import urllib.request as request\n",
    "import re\n",
    "import html\n",
    "import sys, os\n",
    "import pickle\n",
    "sys.path.append(\"C:\\\\Program Files\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\")\n",
    "import tweepy\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "urlstr = \"https://uk-air.defra.gov.uk/latest/currentlevels?view=site#L\"\n",
    "shorturlstr = \"https://goo.gl/ZpELjS\"\n",
    "\n",
    "urlWHO = \"http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf\"\n",
    "\n",
    "sitename = b'Liverpool'\n",
    "\n",
    "mgm3 = '\\u03BCgm\\u207B\\u00B3'\n",
    "O3, NO2, SO2, PM25, PM100 = \"O\\u2083\", \"NO\\u2082\", \"SO\\u2082\", \"PM\\u2082\\u2085\", \"PM\\u2081\\u2080\\u2080\"\n",
    "guides = {O3:100, NO2:200, SO2:20, PM25:25, PM100:50} # source: http://apps.who.int/iris/bitstream/10665/69477/1/WHO_SDE_PHE_OEH_06.02_eng.pdf  \n",
    "meansWHO = {O3:'8h', NO2:'1h', SO2:'10m', PM25:'24h', PM100:'24h'}\n",
    "meansDEFRA = {O3:'8h', NO2:'1h', SO2:'max 15m', PM25:'24h', PM100:'24h'}\n",
    "\n",
    "\n",
    "def tweet(status, replyto=None):\n",
    "    if not status:\n",
    "        return\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret = pickle.load(open(\"apikeys.bin\", \"rb\")) \n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', status)\n",
    "    print(\"urls = \", urls)\n",
    "    # take out all url texts from status for count, all urls count as 23\n",
    "    rstat = status\n",
    "    for u in urls:\n",
    "        rstat = rstat.replace(u, '')\n",
    "    nchars = len(rstat) + 23 * len(urls)\n",
    "    if nchars > 140:\n",
    "        print(\"Tweet too long\")\n",
    "        print(status)\n",
    "\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    if replyto:\n",
    "        stat = api.update_status(status=status, in_reply_to_status_id=replyto.id)\n",
    "    else:\n",
    "        stat = api.update_status(status=status)\n",
    "    return stat\n",
    "    \n",
    "def compose(day, clock, reading):    \n",
    "    status = [\"%s, %s (%s)\" % (day, clock, mgm3)]\n",
    "    skeys = list(reading.keys())\n",
    "    skeys.sort()\n",
    "    for k in skeys:\n",
    "        status.append(\"%s: %.0f %s\" % (k, reading[k][0], reading[k][1]))\n",
    "    status.append(\"%s\" % shorturlstr)\n",
    "    status = '\\n'.join(status)\n",
    "    return status\n",
    "\n",
    "def composeAboveTweet(day, clock, above, origtweetstat):\n",
    "    status = []\n",
    "    for k in above:\n",
    "        # count hours above\n",
    "        print(\"In composeAboveTweet\", k, above[k])\n",
    "        lday, lclock, lvalue = above[k][0]\n",
    "        if lday == day and lclock == clock:\n",
    "            stat = []\n",
    "            # count hours above\n",
    "            nhours = 1\n",
    "            for lday, lclock, lvalue in above[k][1:]:\n",
    "                if int(lclock[:lclock.index(':')]) + nhours == int(clock[:clock.index(':')]):\n",
    "                    nhours += 1\n",
    "                else:\n",
    "                    break\n",
    "            stat.append(\"@lpoolcouncil @DefraUKAir @LiverpoolFoE: %s %dh above @WHO guide (%.0f%s %s-mean %s) #airpollution #liverpool\" % \n",
    "                        (k, nhours, guides[k], mgm3, meansWHO[k], urlWHO))\n",
    "            if meansWHO[k] != meansDEFRA[k]:\n",
    "                stat.append(\"(Note #DEFRA data is %s mean)\" % meansDEFRA[k])            \n",
    "            status.append('\\n'.join(stat))\n",
    "    return status\n",
    "        \n",
    "\n",
    "\n",
    "def scrape():\n",
    "    f = request.urlopen(urlstr)\n",
    "\n",
    "    r = f.read()\n",
    "    g = re.search(b\".*<tr>.*(%s.*?)</tr>\" % sitename, r, re.DOTALL)\n",
    "    #print(g.group(1))\n",
    "\n",
    "    # split into <td></td>\n",
    "    row = g.group(1)\n",
    "    #print(\"row = %s\\n\" % row)\n",
    "\n",
    "    # date and time\n",
    "    dategroups = re.search(b\".*<td>(.*?)<br.*?>(.*?)</td>\", row, re.DOTALL)\n",
    "    day = dategroups.group(1).decode(\"utf-8\")\n",
    "    clock = dategroups.group(2).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "    # data\n",
    "    cols = re.findall(b\"<span.*?>(.*?)</span>\", row, re.DOTALL)\n",
    "    assert len(cols) == 5\n",
    "    units = [O3, NO2, SO2, PM25, PM100]\n",
    "    datanums = []\n",
    "    for v in cols:\n",
    "        if b' ' in v:\n",
    "            value = float(v[:v.index(b' ')])\n",
    "        else:\n",
    "            value = float(v[:v.index(b'&')])\n",
    "        nv = v.replace(b'&nbsp;', b' ')\n",
    "        ix = re.match(b\".*?(\\(.*?\\))\", nv).group(1)\n",
    "        datanums.append((value, ix.decode(\"utf-8\")))\n",
    "\n",
    "    reading = dict(zip(units, datanums))\n",
    "    return day, clock, reading\n",
    "\n",
    "def loadReadings():\n",
    "    fall = \"allreadings.bin\"\n",
    "    allreadings = deque()\n",
    "    if os.path.isfile(fall):\n",
    "        allreadings = pickle.load(open(fall, \"rb\"))\n",
    "    return allreadings\n",
    "\n",
    "def pickleReadings(allreading):\n",
    "    fall = \"allreadings.bin\"\n",
    "    pickle.dump(allreadings, open(fall, \"wb\"))\n",
    "    \n",
    "def compareWHO(allreadings):\n",
    "    above = {}\n",
    "    for (day, clock, reading) in allreadings:\n",
    "        for k in guides:\n",
    "            if reading[k][0] > guides[k]:\n",
    "                if k not in above:\n",
    "                    above[k] = []\n",
    "                above[k].append((day,clock, reading[k][0]))\n",
    "    return above\n",
    "\n",
    "\n",
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    stat = tweet(\"TTEESSTT\")\n",
    "    print(stat)\n",
    "    #tweet(\"In reply to: TEST3\", stat)\n",
    "\n",
    "else:\n",
    "    day, clock, reading = scrape()\n",
    "    status = compose(day, clock, reading)\n",
    "    rtweet = tweet(status)\n",
    "\n",
    "    allreadings = loadReadings()\n",
    "    allreadings.appendleft((day, clock, reading))\n",
    "    pickleReadings(allreadings)\n",
    "\n",
    "    # compare with WHO recommendations\n",
    "    r = compareWHO(allreadings)\n",
    "    if r:\n",
    "        stats = composeAboveTweet(day, clock, r, rtweet)\n",
    "        for s in stats:\n",
    "            tweet(s, replyto=rtweet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
